

\section{Wednesday}\index{Wednesday_lecture}
\subsection{Condition for existence and uniqueness of first ODE (Include non-linear)}Consider $\left \{	\begin{gathered}
y^\prime=f(t,y)	\\
y(t_0)=y_0.
\end{gathered}\right.$ Recall that when $f$ and $\frac{\partial f}{\partial y}$ are continuous on a nbhd of $(t_0,y_0)$, there exists a unique solution to (IVP).
There comes a question: what are the least conditions required for existence of a solution?\\ It only require $f$ is continuous.\\
\begin{theorem}
Suppose that $f(t,y)$ is continuous on $\mathcal{R}=[t_0-a,t_0+a]\times[y_0-b,y_0+b]$ $a,b>0$. Then IVP has a solution $y=y(t)$ in $(t_0-\alpha,t_0+\alpha)$ where $\alpha=min\{a,\frac{b}{m}\}$ $M=max_\mathcal{R}|f(t,y)|$
\end{theorem}

Proof outline:\\
1. $\{y_0, y_1, \dots, y_n,\dots\}$ is an equicontinuous family.\\
2. Arzela-Ascoli theorem implies there exists a convergent subsequence and the limit is our sol.

\begin{proof}[Manual Script]
$y_n$ is in $[y_0-b,y_0+b]$ which means it is bounded uniformly. In addition, for any $\varepsilon$ there exists $\delta=\frac{\varepsilon}{M}$ such that when $|t_2-t_1|<\delta$, $y_n(t_1)-y_n(t_2)=\int_{t_1}^{t_2}f(s,y_{n-1}(s))\diff s\leq M\int_{t_1}^{t_2}1\diff s=M(t_2-t_1)=\varepsilon$. Therefore, $\{y_0,y_1,\dots,y_n,\dots\}$ is indeed an equicontinuous. Then by Arzela-Ascoli theorem there exists a uniformly convergent subsequence.
\end{proof}
Rather nothing more than 1. is proved, if you are interested in the complete prove, check {\it Theory of ordinary differential equations} by Coddington \& Levinson \href{https://ptvtpqa.files.wordpress.com/2013/12/coddington-e-levinson-n-theory-of-ordinary-differential-equations.pdf}{The link of the book}.
Though the construct of sequence of approximating functions is slightly different, the main idea is the same.\\

\paragraph{Condition for uniqueness to hold}Replace ``$\frac{\partial f}{\partial y}$'' being continuous on $\mathcal{R}$ ``by'' $|f(s,y_1)-f(s,y_2)|\leq L|y_1-y_2|$ (Lipshitz continuity)

\begin{remark}
\begin{itemize}
\item 
Intuition of uniqueness\\
Consider a simple case
 $\left \{	\begin{gathered}
y^\prime=f(t,y)	\\
y(0)=0.
\end{gathered}\right.$ \\$y\equiv0$ where $f(0)=0$, $f(y)\geq0$, $f(y)\leq Cy$, for $y\geq0$ small.( $y$ increases in $t>0$ small).\\
Suppose $\exists$ sol$\neq0$, $t\geq0$\\
\[\begin{aligned}0\leq y(t)&=\int_0^tf(y(s))\diff s\leq C\int_0^ty(s)\diff s\\
&\leq Cy(t)\int_0^t1\diff s\\
&=Cty(t)
\end{aligned}
\]
\[\Rightarrow\quad 1\leq Ct 
\]
For $C$ is a constant, when $t<\frac{1}{2C}$, the above inequality doesn't hold. Therefore, a contradiction appears. This implies that the only solution must be identically 0.

$f(y)\leq Cy$ is because of $f^\prime(0)$. The derivative exists implies that $f(y)$ can be expressed linearly. Therefore, the derivative at initial value point leads to the uniqueness of the solution. (Personal interpretation of professor's words, check that.)
\item
Picard iteration\\
This is an example shows how picard iteration is working.
\begin{example}
 $\left \{	\begin{gathered}
y^\prime=1+y^2	\\
y(0)=0.
\end{gathered}\right.$ $\qquad\Rightarrow y=\tan t$
\[
\begin{array}{lll}
y_0(t)&\equiv0&\\
y_1(t)&=\int_0^tf(s,y_0)\diff s&=\int_0^t1\diff s=t\\
y_2(t)&=\int_0^tf(s,y_1)\diff s&=\int_0^t1+s^2\diff s=t+\frac{1}{3}t^3\\
y_3(t)&=\int_0^t(1+y_2^2)\diff s&=\int_0^t1+s^2+\frac{2}{3}s^4+\frac{1}{9}s^6\diff s\\
&&=t+\frac{1}{3}t^3+\frac{2}{15}t^5+\frac{1}{63}t^7\\
y_4&=\dots
\end{array}
\]
\[tant=t+\frac{1}{3}t^3+\frac{2}{15}t^5+\frac{17}{315}t^7+\frac{62}{2835}t^9=\dots
\]
It's easy to observe that the approximating function is meaningful at any point. While, the solution is only meaningful at$(-\frac{\pi}{2},\frac{\pi}{2})$.( Shown before, check note.)\\

PS: Just show you how complicated the taylor expansion of a common function can be.
\[\tan t=\sum_{n=0}^{\infty}\frac{(-1)^n2^{2n+2}(2^{2n+2}-1)B_{2n+2}t^{2n+1}}{(2n+2)}
\]
where $B_n$ is a Bernoulli number defined by $\frac{x}{e^x-1}=\sum_{n=0}^{\infty}\frac{B_n}{n!}x^n$
\end{example}
\end{itemize}
\end{remark}
\subsection{Exact Equations}
``We can solve all differential equations which are, or can be put, in the form
\[\frac{\diff}{\diff t}\phi(t,y)=0
\]
for some function $\phi(t,y)$
``(Martin, p58).
If we want such $\phi$ exists, it has to satisfies  $\frac{\partial M}{\partial y}=\frac{\partial N}{\partial t}$. (That
s why the definition requires this.) 
\begin{definition}The differential equation
\[M(t,y)+N(t,y)\frac{\diff y}{\diff t}=0
\]
is said to be exact if $\frac{\partial M}{\partial y}=\frac{\partial N}{\partial t}$.
\end{definition}
\begin{theorem}
Given $M(t,y)$, $N(t,y)\in C^\prime(\bar{\mathcal{R}})$ where $\bar{\mathcal{R}}=[a,b]\times[c,d]$. Then $\exists~\phi$ s.t. $M=\phi_t$ $N=\phi_y$ if and only if $M_y=N_t$
\end{theorem}
\begin{proof}
($\Rightarrow$) $M=\phi_t$ and $N=\phi_y$ $\Rightarrow~M_y=\phi_{ty}=\phi_{yt}=N_t$ 
($\Leftarrow$)  let $\phi(t,y)=\int M(t,y)\diff t+\int[N(t,y)-\int\frac{\partial M}{\partial y}\diff t]\diff y$ (If you are wondering why there is such a come-from-nowhere function, see textbook.)
\[\phi_t=M(t,y)+\underbrace{\int N_t\diff y-\int M_y\diff y}_{=0}
\]
\[\phi_y=\int M_y\diff t+N-\int M_y\diff t=N
\]
\end{proof}

Integrating factor might be helpful to find $\phi(t,y)$, need
\[(\mu M)_y=(\mu N)_t
\]
\[\mu M_y+\mu_yM=\mu_tN+\mu N_t
\]
This is a partial differential equation. (We want $\mu$) It seems we have dig a hole for ourselves. However, if $\mu$ is a function only concern one of $t$ and $y$, life would be easier.\\
If $\mu=\mu(t)$ $\mu(M_y-N_t)=\mu_tN$, $\frac{\mu_t}{\mu}=\frac{M_y-N_t}{N}$. When $\mu=\mu(y)$, the situation is similiar. (There is a example 6 in p65 in textbook.)
